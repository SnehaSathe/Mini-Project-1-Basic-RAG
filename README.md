# ğŸ§  Mini Project 1: Retrieval-Augmented Generation (RAG) System

A hands-on **mini project to deeply understand core RAG concepts** using modern LangChain (v1.x), FAISS vector database, HuggingFace embeddings, and a local LLM via Ollama.

This project is designed as a **foundation-building exercise** before moving to advanced, production-grade RAG systems.

---

## ğŸš€ Project Objective

To build an **end-to-end RAG pipeline** that:

* Ingests PDF documents
* Chunks and embeds text efficiently
* Stores embeddings in FAISS
* Retrieves relevant context
* Uses a local LLM to answer user queries **only from retrieved context**

---

## ğŸ—ï¸ Architecture Overview

```
PDF Documents
     â†“
Document Loader (PyMuPDF)
     â†“
Text Chunking (RecursiveCharacterTextSplitter)
     â†“
Embeddings (HuggingFace MiniLM)
     â†“
Vector Store (FAISS)
     â†“
Retriever (Similarity Search)
     â†“
Prompt Template
     â†“
Local LLM (Ollama - Llama3)
     â†“
Grounded Answer
```

---

## ğŸ§° Tech Stack

* **Python 3.12**
* **LangChain v1.x (LCEL / Runnable chains)**
* **FAISS** â€“ vector database
* **HuggingFace Embeddings** â€“ `sentence-transformers/all-MiniLM-L6-v2`
* **Ollama** â€“ local LLM (`llama3`)
* **PyMuPDF** â€“ PDF loading

---

## ğŸ“ Project Structure

```
MINI_PROJECT1/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample.pdf
â”‚
â”œâ”€â”€ ingest.py        # Document ingestion + FAISS index creation
â”œâ”€â”€ rag_qa.py        # RAG-based question answering
â”œâ”€â”€ faiss_index/     # Stored vector index (auto-generated)
â”‚   â”œâ”€â”€ index.faiss
â”‚   â””â”€â”€ index.pkl
â”‚
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Create virtual environment (recommended)

```bash
python -m venv mini1
mini1\Scripts\activate
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install langchain langchain-core langchain-community \
langchain-ollama langchain-text-splitters \
faiss-cpu sentence-transformers pymupdf
```

### 3ï¸âƒ£ Install & run Ollama

```bash
ollama pull llama3
ollama run llama3
```

---

## ğŸ“¥ Ingest Documents

Place PDFs inside the `data/` folder.

Run:

```bash
python ingest.py
```

This will:

* Load PDF pages
* Split text into chunks
* Generate embeddings
* Save FAISS index locally

---

## ğŸ’¬ Ask Questions (RAG)

Run:

```bash
python rag_qa.py
```

Example queries:

* `What is this document about?`
* `Explain encoder and decoder stacks`
* `Summarize key concepts`

The system answers **only using retrieved context**.

---

## Output 
![alt text](image.png)

## ğŸ§  Key Learnings

* Difference between **LLMs vs Embedding Models**
* Why embedding model consistency is critical
* How FAISS similarity search works
* Modern LangChain **LCEL / Runnable** pattern
* Common real-world RAG errors and fixes

---

## ğŸ” Common Issues Solved

* LangChain v1.x breaking changes
* FAISS dimension mismatch errors
* Ollama vs HuggingFace model confusion
* Performance optimization during ingestion

---

## ğŸ‘©â€ğŸ’» Author

**Sneha Sathe**
Aspiring GenAI / AI Engineer | RAG | LangChain | LLMs

---

â­ If you find this useful, feel free to star the repo!
